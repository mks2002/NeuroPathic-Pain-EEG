{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d41ec22",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Notebook: Subject-level Stratified Train/Test + ML baseline pipeline\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import (accuracy_score, balanced_accuracy_score,\n",
    "                             classification_report, confusion_matrix)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, HistGradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set_theme(style='whitegrid')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22ace7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pain scores mapping\n",
    "PAIN_SCORE = {\n",
    "    0: 7, 1: 4, 2: 3, 3: 8, 4: 5, 5: 2, 6: 7, 7: 3, 8: 4, 9: 9,\n",
    "    10: 3, 11: 6, 13: 3, 14: 3, 15: 8, 16: 5, 18: 5, 19: 8, 20: 7,\n",
    "    21: 6, 22: 7, 23: 6, 24: 9, 25: 8, 26: 0, 27: 1, 30: 3, 31: 9,\n",
    "    33: 6, 35: 1, 37: 0, 38: 7, 39: 8, 40: 4, 41: 7, 43: 6\n",
    "}\n",
    "\n",
    "def pain_label(score):\n",
    "    if score in (0, 1, 2):\n",
    "        return \"low\"\n",
    "    elif score in (3, 4, 5, 6):\n",
    "        return \"mid\"\n",
    "    else:\n",
    "        return \"high\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14e66d72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONFIG:\n",
      " DATA_DIR: FP1_FeatureData_Simple_Overlapping\n",
      " OUTPUT_DIR: ml_results_subject_split\n",
      " TEST_SIZE: 0.2\n",
      " RANDOM_STATE: 42\n"
     ]
    }
   ],
   "source": [
    "#  CONFIG\n",
    "DATA_DIR = Path(\"FP1_FeatureData_Simple_Overlapping\")   # <-- change to your folder with per-subject CSVs\n",
    "OUTPUT_DIR = Path(\"ml_results_subject_split\")\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "TEST_SIZE = 0.2   # fraction of subjects for test\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# Provide your grouping mapping (from your message)\n",
    "GROUPS = {\n",
    "    \"low\":    [5, 26, 27, 35, 37],\n",
    "    \"mid\":    [1,2,4,7,8,10,11,13,14,16,18,21,23,30,33,40,43],\n",
    "    \"high\":   [0,3,6,9,15,19,20,22,24,25,31,38,39,41]\n",
    "}\n",
    "\n",
    "# Flatten mapping into id->label for safety\n",
    "PAIN_LABEL_MAP = {}\n",
    "for lab, ids in GROUPS.items():\n",
    "    for i in ids:\n",
    "        PAIN_LABEL_MAP[i] = lab\n",
    "\n",
    "print(\"CONFIG:\")\n",
    "print(\" DATA_DIR:\", DATA_DIR)\n",
    "print(\" OUTPUT_DIR:\", OUTPUT_DIR)\n",
    "print(\" TEST_SIZE:\", TEST_SIZE)\n",
    "print(\" RANDOM_STATE:\", RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c6e035",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e96b3cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 36 subject CSVs.\n",
      "IDs present in folder: 36\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# READ CSVs & CHECK MAPPING\n",
    "# Gather all ID*_feature.csv files\n",
    "csv_files = sorted(DATA_DIR.glob(\"ID*_feature.csv\"))\n",
    "if not csv_files:\n",
    "    raise FileNotFoundError(f\"No CSVs found in {DATA_DIR}. Check path and filenames.\")\n",
    "\n",
    "# Build per-subject dataframe mapping\n",
    "subjects = []\n",
    "for p in csv_files:\n",
    "    # parse ID number from filename (assumes 'ID<number>_feature.csv')\n",
    "    stem = p.stem\n",
    "    # extract first integer in the stem\n",
    "    nums = ''.join([c if c.isdigit() else ' ' for c in stem]).split()\n",
    "    if not nums:\n",
    "        print(\"Cannot parse ID from\", p.name)\n",
    "        continue\n",
    "    sid = int(nums[0])\n",
    "    subjects.append({'id': sid, 'path': p})\n",
    "subj_df = pd.DataFrame(subjects).sort_values('id').reset_index(drop=True)\n",
    "print(f\"Found {len(subj_df)} subject CSVs.\")\n",
    "\n",
    "\n",
    "\n",
    "# Verify grouping consistency vs present IDs\n",
    "present_ids = set(subj_df['id'].tolist())\n",
    "mapped_ids = set(PAIN_LABEL_MAP.keys())\n",
    "missing_in_map = present_ids - mapped_ids\n",
    "missing_in_folder = mapped_ids - present_ids\n",
    "print(\"IDs present in folder:\", len(present_ids))\n",
    "if missing_in_map:\n",
    "    print(\"Warning: these subject IDs have no label mapping (PAIN_LABEL_MAP):\", sorted(missing_in_map))\n",
    "if missing_in_folder:\n",
    "    print(\"Note: mapping contains IDs not present as CSV:\", sorted(missing_in_folder))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c8259085",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject-level counts by label: {'high': 14, 'mid': 17, 'low': 5}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Build subject -> label (from CSV label column if exists else from PAIN_LABEL_MAP)\n",
    "subj_labels = {}\n",
    "for idx, row in subj_df.iterrows():\n",
    "    sid = int(row['id'])\n",
    "    # try to read the CSV's label column (first row) to confirm\n",
    "    df_tmp = pd.read_csv(row['path'], nrows=2)  # small peek\n",
    "    if 'label' in df_tmp.columns:\n",
    "        label_val = df_tmp['label'].iloc[0]\n",
    "        subj_labels[sid] = str(label_val)\n",
    "    else:\n",
    "        # fallback to provided mapping\n",
    "        if sid in PAIN_LABEL_MAP:\n",
    "            subj_labels[sid] = PAIN_LABEL_MAP[sid]\n",
    "        else:\n",
    "            raise ValueError(f\"Cannot determine label for subject ID {sid}. Add mapping or ensure 'label' column in CSV.\")\n",
    "        \n",
    "        \n",
    "\n",
    "# Print counts per class (subject-level)\n",
    "from collections import Counter\n",
    "cnt = Counter(subj_labels.values())\n",
    "print(\"Subject-level counts by label:\", dict(cnt))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb79703b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subjects -> Train: 28  Test: 8\n",
      "Train label counts: {np.str_('high'): 11, np.str_('mid'): 13, np.str_('low'): 4}\n",
      "Test  label counts: {np.str_('mid'): 4, np.str_('high'): 3, np.str_('low'): 1}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# BUILD SUBJECT LIST & STRATIFIED SPLIT\n",
    "# We'll split subjects (not windows) while stratifying on the subject-level label\n",
    "subject_ids = np.array(sorted(list(subj_labels.keys())))\n",
    "subject_labels = np.array([subj_labels[s] for s in subject_ids])\n",
    "\n",
    "# train/test split of subjects\n",
    "train_sids, test_sids, y_train_sub, y_test_sub = train_test_split(\n",
    "    subject_ids, subject_labels,\n",
    "    test_size=TEST_SIZE, random_state=RANDOM_STATE, stratify=subject_labels\n",
    ")\n",
    "\n",
    "print(\"Subjects -> Train:\", len(train_sids), \" Test:\", len(test_sids))\n",
    "print(\"Train label counts:\", dict(Counter(y_train_sub)))\n",
    "print(\"Test  label counts:\", dict(Counter(y_test_sub)))\n",
    "\n",
    "# Save lists\n",
    "pd.DataFrame({'subject_id': train_sids}).to_csv(OUTPUT_DIR / 'train_subjects.csv', index=False)\n",
    "pd.DataFrame({'subject_id': test_sids}).to_csv(OUTPUT_DIR / 'test_subjects.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "877a6a1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row-level: train rows = 6688 , test rows = 1912\n",
      "Row-level label distribution (train):\n",
      " label\n",
      "mid     3107\n",
      "high    2627\n",
      "low      954\n",
      "Name: count, dtype: int64\n",
      "Row-level label distribution (test):\n",
      " label\n",
      "mid     956\n",
      "high    717\n",
      "low     239\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ASSEMBLE ROW-LEVEL DATA (no leakage)\n",
    "# Load all rows for train subjects and test subjects. Use subj_id column to select.\n",
    "def load_subject_csv(sid):\n",
    "    p = DATA_DIR / f\"ID{sid}_feature.csv\"\n",
    "    if not p.exists():\n",
    "        raise FileNotFoundError(f\"Expected file {p} not found.\")\n",
    "    df = pd.read_csv(p)\n",
    "    # ensure subj_id column exists (if not, add)\n",
    "    if 'subj_id' not in df.columns:\n",
    "        df['subj_id'] = sid\n",
    "    return df\n",
    "\n",
    "train_rows = []\n",
    "for sid in train_sids:\n",
    "    df = load_subject_csv(sid)\n",
    "    train_rows.append(df)\n",
    "test_rows = []\n",
    "for sid in test_sids:\n",
    "    df = load_subject_csv(sid)\n",
    "    test_rows.append(df)\n",
    "\n",
    "train_df = pd.concat(train_rows, ignore_index=True)\n",
    "test_df = pd.concat(test_rows, ignore_index=True)\n",
    "print(\"Row-level: train rows =\", len(train_df), \", test rows =\", len(test_df))\n",
    "\n",
    "# quick class balance check (row-level)\n",
    "print(\"Row-level label distribution (train):\\n\", train_df['label'].value_counts())\n",
    "print(\"Row-level label distribution (test):\\n\", test_df['label'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ec13bb41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of feature columns detected: 52\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['ml_results_subject_split\\\\preprocessing_pipeline.joblib']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# PREPROCESS: feature selection, impute, scale\n",
    "# Remove meta columns we don't want as features\n",
    "META_COLS = ['subj_id', 'window_idx', 'label', 'pain_score']\n",
    "# ensure these exist\n",
    "for c in META_COLS:\n",
    "    if c not in train_df.columns:\n",
    "        print(f\"Warning: expected meta column {c} not found in data.\")\n",
    "\n",
    "\n",
    "# Feature columns = all numeric columns except meta\n",
    "feat_cols = [c for c in train_df.columns if c not in META_COLS]\n",
    "print(\"Number of feature columns detected:\", len(feat_cols))\n",
    "\n",
    "X_train = train_df[feat_cols].values\n",
    "y_train = train_df['label'].values\n",
    "X_test = test_df[feat_cols].values\n",
    "y_test = test_df['label'].values\n",
    "\n",
    "# Encode labels to integers for classifiers\n",
    "le = LabelEncoder().fit(y_train)  # fit on train labels\n",
    "y_train_enc = le.transform(y_train)\n",
    "y_test_enc = le.transform(y_test)\n",
    "\n",
    "# Preprocessing pipeline (impute median + standard scale)\n",
    "preproc = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "\n",
    "X_train_p = preproc.fit_transform(X_train)\n",
    "X_test_p = preproc.transform(X_test)\n",
    "\n",
    "# Save preproc\n",
    "joblib.dump(preproc, OUTPUT_DIR / 'preprocessing_pipeline.joblib')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a0ebd3ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6688, 52) (1912, 52)\n",
      "(6688,) (1912,)\n",
      "(6688, 52) (1912, 52)\n",
      "(6688,) (1912,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, X_test.shape)\n",
    "print(y_train.shape, y_test.shape)\n",
    "\n",
    "# after preprocessing\n",
    "print(X_train_p.shape, X_test_p.shape)\n",
    "print(y_train_enc.shape, y_test_enc.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "15d3a30b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost available and added.\n",
      "\n",
      "==================================================\n",
      "Training: LogisticRegression\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        high       0.20      0.11      0.14       717\n",
      "         low       0.01      0.01      0.01       239\n",
      "         mid       0.44      0.62      0.51       956\n",
      "\n",
      "    accuracy                           0.35      1912\n",
      "   macro avg       0.22      0.25      0.22      1912\n",
      "weighted avg       0.30      0.35      0.31      1912\n",
      "\n",
      "Accuracy: 0.3530334728033473  Balanced Accuracy: 0.24628079962807994\n",
      "\n",
      "==================================================\n",
      "Training: RandomForest\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        high       0.17      0.26      0.20       717\n",
      "         low       0.05      0.03      0.03       239\n",
      "         mid       0.28      0.20      0.24       956\n",
      "\n",
      "    accuracy                           0.20      1912\n",
      "   macro avg       0.17      0.16      0.16      1912\n",
      "weighted avg       0.21      0.20      0.20      1912\n",
      "\n",
      "Accuracy: 0.200836820083682  Balanced Accuracy: 0.16166899116689912\n",
      "\n",
      "==================================================\n",
      "Training: GradientBoosting\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        high       0.22      0.30      0.26       717\n",
      "         low       0.02      0.01      0.01       239\n",
      "         mid       0.39      0.34      0.37       956\n",
      "\n",
      "    accuracy                           0.29      1912\n",
      "   macro avg       0.21      0.22      0.21      1912\n",
      "weighted avg       0.28      0.29      0.28      1912\n",
      "\n",
      "Accuracy: 0.2850418410041841  Balanced Accuracy: 0.2171083217108322\n",
      "\n",
      "==================================================\n",
      "Training: HistGradientBoosting\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        high       0.21      0.31      0.25       717\n",
      "         low       0.05      0.03      0.04       239\n",
      "         mid       0.31      0.24      0.27       956\n",
      "\n",
      "    accuracy                           0.24      1912\n",
      "   macro avg       0.19      0.19      0.19      1912\n",
      "weighted avg       0.24      0.24      0.23      1912\n",
      "\n",
      "Accuracy: 0.23744769874476987  Balanced Accuracy: 0.19130636913063692\n",
      "\n",
      "==================================================\n",
      "Training: SVC\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        high       0.22      0.22      0.22       717\n",
      "         low       0.02      0.01      0.01       239\n",
      "         mid       0.37      0.40      0.39       956\n",
      "\n",
      "    accuracy                           0.29      1912\n",
      "   macro avg       0.20      0.21      0.21      1912\n",
      "weighted avg       0.27      0.29      0.28      1912\n",
      "\n",
      "Accuracy: 0.28661087866108786  Balanced Accuracy: 0.21187819618781964\n",
      "\n",
      "==================================================\n",
      "Training: KNN\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        high       0.25      0.31      0.28       717\n",
      "         low       0.01      0.01      0.01       239\n",
      "         mid       0.43      0.38      0.40       956\n",
      "\n",
      "    accuracy                           0.31      1912\n",
      "   macro avg       0.23      0.23      0.23      1912\n",
      "weighted avg       0.31      0.31      0.31      1912\n",
      "\n",
      "Accuracy: 0.30857740585774057  Balanced Accuracy: 0.2337284983728498\n",
      "\n",
      "==================================================\n",
      "Training: XGBoost\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        high       0.23      0.30      0.26       717\n",
      "         low       0.05      0.03      0.04       239\n",
      "         mid       0.38      0.33      0.35       956\n",
      "\n",
      "    accuracy                           0.28      1912\n",
      "   macro avg       0.22      0.22      0.22      1912\n",
      "weighted avg       0.28      0.28      0.28      1912\n",
      "\n",
      "Accuracy: 0.27981171548117156  Balanced Accuracy: 0.21896792189679223\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# MODEL LIST\n",
    "models = {\n",
    "    \"LogisticRegression\": LogisticRegression(max_iter=2000, random_state=RANDOM_STATE),\n",
    "    \"RandomForest\": RandomForestClassifier(n_estimators=200, random_state=RANDOM_STATE, n_jobs=-1),\n",
    "    \"GradientBoosting\": GradientBoostingClassifier(n_estimators=200, random_state=RANDOM_STATE),\n",
    "    \"HistGradientBoosting\": HistGradientBoostingClassifier(random_state=RANDOM_STATE),\n",
    "    \"SVC\": SVC(probability=True, random_state=RANDOM_STATE),\n",
    "    \"KNN\": KNeighborsClassifier(n_neighbors=5)\n",
    "}\n",
    "# Try to add XGBoost if available\n",
    "try:\n",
    "    from xgboost import XGBClassifier\n",
    "    models[\"XGBoost\"] = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=RANDOM_STATE)\n",
    "    print(\"XGBoost available and added.\")\n",
    "except Exception:\n",
    "    print(\"XGBoost not available (skipping).\")\n",
    "\n",
    "# %% TRAIN, EVALUATE, PLOT\n",
    "results = []\n",
    "for name, clf in models.items():\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"Training:\", name)\n",
    "    clf.fit(X_train_p, y_train_enc)\n",
    "    # Save model\n",
    "    joblib.dump(clf, OUTPUT_DIR / f\"{name}.joblib\")\n",
    "    y_pred = clf.predict(X_test_p)\n",
    "    y_prob = None\n",
    "    if hasattr(clf, \"predict_proba\"):\n",
    "        y_prob = clf.predict_proba(X_test_p)\n",
    "\n",
    "    acc = accuracy_score(y_test_enc, y_pred)\n",
    "    bacc = balanced_accuracy_score(y_test_enc, y_pred)\n",
    "    report = classification_report(y_test_enc, y_pred, target_names=le.classes_, output_dict=True)\n",
    "    report_text = classification_report(y_test_enc, y_pred, target_names=le.classes_)\n",
    "    print(report_text)\n",
    "    print(\"Accuracy:\", acc, \" Balanced Accuracy:\", bacc)\n",
    "\n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(y_test_enc, y_pred)\n",
    "    fig, ax = plt.subplots(figsize=(5,4))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', xticklabels=le.classes_, yticklabels=le.classes_, cmap='Blues', ax=ax)\n",
    "    ax.set_xlabel('Predicted')\n",
    "    ax.set_ylabel('True')\n",
    "    ax.set_title(f\"{name} — Confusion Matrix\")\n",
    "    plt.tight_layout()\n",
    "    fig_path = OUTPUT_DIR / f\"{name}_confusion_matrix.png\"\n",
    "    fig.savefig(fig_path)\n",
    "    plt.close(fig)\n",
    "\n",
    "    # Collect numeric metrics summary\n",
    "    results.append({\n",
    "        'model': name,\n",
    "        'accuracy': acc,\n",
    "        'balanced_accuracy': bacc,\n",
    "        'precision_macro': report['macro avg']['precision'],\n",
    "        'recall_macro': report['macro avg']['recall'],\n",
    "        'f1_macro': report['macro avg']['f1-score'],\n",
    "        'n_train_subjects': len(train_sids),\n",
    "        'n_test_subjects': len(test_sids),\n",
    "        'n_train_rows': len(train_df),\n",
    "        'n_test_rows': len(test_df)\n",
    "    })\n",
    "\n",
    "    # Save detailed report\n",
    "    with open(OUTPUT_DIR / f\"{name}_classification_report.txt\", 'w') as fh:\n",
    "        fh.write(report_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6098d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save results summary\n",
    "results_df = pd.DataFrame(results).sort_values('f1_macro', ascending=False)\n",
    "results_df.to_csv(OUTPUT_DIR / \"model_results_summary.csv\", index=False)\n",
    "print(\"\\nSummary:\")\n",
    "print(results_df)\n",
    "\n",
    "# %% EXTRA: show per-class support in test set and mapping\n",
    "print(\"\\nTest set per-class row support:\", test_df['label'].value_counts().to_dict())\n",
    "print(\"Label encoding mapping (int -> label):\", dict(enumerate(le.classes_)))\n",
    "\n",
    "# EOF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b1bd326",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603ff533",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from tensorflow import keras\n",
    "\n",
    "# ==========================\n",
    "# Grad-CAM heatmap function\n",
    "# ==========================\n",
    "def make_gradcam_heatmap(img_array, model, last_conv_layer_name, pred_index=None):\n",
    "    \"\"\"\n",
    "    Generate a Grad-CAM heatmap for a given image and model.\n",
    "    \n",
    "    Args:\n",
    "        img_array (np.ndarray): Input image array of shape (1, H, W, 3)\n",
    "            with pixel values in [0, 255].\n",
    "        model (tf.keras.Model): The trained model containing a Rescaling layer.\n",
    "        last_conv_layer_name (str): Name of the last convolutional layer.\n",
    "        pred_index (int, optional): Class index to visualize. Defaults to predicted class.\n",
    "    \n",
    "    Returns:\n",
    "        np.ndarray: Normalized heatmap (H, W) values in [0, 1].\n",
    "    \"\"\"\n",
    "    # Build a model mapping input to activations + output\n",
    "    grad_model = tf.keras.models.Model(\n",
    "        [model.inputs],\n",
    "        [model.get_layer(last_conv_layer_name).output, model.output]\n",
    "    )\n",
    "\n",
    "    # Record gradients\n",
    "    with tf.GradientTape() as tape:\n",
    "        conv_outputs, predictions = grad_model(img_array)\n",
    "        if pred_index is None:\n",
    "            pred_index = tf.argmax(predictions[0])\n",
    "        class_channel = predictions[:, pred_index]\n",
    "\n",
    "    # Compute gradient of class output wrt conv layer\n",
    "    grads = tape.gradient(class_channel, conv_outputs)\n",
    "\n",
    "    # Mean intensity of gradients for each feature map\n",
    "    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n",
    "\n",
    "    # Weight each feature map by its importance\n",
    "    conv_outputs = conv_outputs[0]\n",
    "    heatmap = conv_outputs @ pooled_grads[..., tf.newaxis]\n",
    "    heatmap = tf.squeeze(heatmap)\n",
    "\n",
    "    # Normalize to [0, 1]\n",
    "    heatmap = tf.maximum(heatmap, 0)\n",
    "    max_val = tf.reduce_max(heatmap)\n",
    "    if max_val > 0:\n",
    "        heatmap /= max_val\n",
    "\n",
    "    return heatmap.numpy()\n",
    "\n",
    "# ==========================================\n",
    "# Function to overlay Grad-CAM on the image\n",
    "# ==========================================\n",
    "def overlay_gradcam(img, heatmap, alpha=0.4, colormap=cv2.COLORMAP_JET):\n",
    "    \"\"\"\n",
    "    Overlay Grad-CAM heatmap on the original image.\n",
    "    \n",
    "    Args:\n",
    "        img (np.ndarray): Original image array (H, W, 3), uint8 [0–255].\n",
    "        heatmap (np.ndarray): Grad-CAM heatmap (H, W), float [0–1].\n",
    "        alpha (float): Transparency factor for blending.\n",
    "        colormap (int): OpenCV colormap to apply.\n",
    "    \n",
    "    Returns:\n",
    "        np.ndarray: RGB image with heatmap overlay.\n",
    "    \"\"\"\n",
    "    # Ensure uint8 image\n",
    "    if img.dtype != np.uint8:\n",
    "        img = np.uint8(255 * img / np.max(img))\n",
    "\n",
    "    # Resize heatmap to image size\n",
    "    heatmap = cv2.resize(heatmap, (img.shape[1], img.shape[0]))\n",
    "\n",
    "    # Convert heatmap to RGB using colormap\n",
    "    heatmap = np.uint8(255 * heatmap)\n",
    "    heatmap = cv2.applyColorMap(heatmap, colormap)\n",
    "    heatmap = cv2.cvtColor(heatmap, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Superimpose heatmap on original image\n",
    "    superimposed_img = cv2.addWeighted(heatmap, alpha, img, 1 - alpha, 0)\n",
    "    return superimposed_img\n",
    "\n",
    "# ==================================================\n",
    "# Display Grad-CAM results for a few sample images\n",
    "# ==================================================\n",
    "def show_gradcam_samples(dataset, class_names, model, last_conv_layer_name, num_images=3):\n",
    "    \"\"\"\n",
    "    Display Grad-CAM visualizations for a few images from a dataset.\n",
    "    \n",
    "    Args:\n",
    "        dataset (tf.data.Dataset): Dataset containing (image, label) pairs.\n",
    "        class_names (list): List of class names in order.\n",
    "        model (tf.keras.Model): Trained model.\n",
    "        last_conv_layer_name (str): Name of the last convolutional layer.\n",
    "        num_images (int): Number of images to visualize.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    for images, labels in dataset.take(1):\n",
    "        for i in range(min(num_images, images.shape[0])):\n",
    "            img = images[i].numpy().astype(\"uint8\")\n",
    "            label = int(labels[i].numpy())\n",
    "\n",
    "            # Expand dims (model includes its own normalization layer)\n",
    "            img_array = np.expand_dims(img, axis=0)\n",
    "\n",
    "            # Generate Grad-CAM\n",
    "            heatmap = make_gradcam_heatmap(img_array, model, last_conv_layer_name)\n",
    "            superimposed_img = overlay_gradcam(img, heatmap)\n",
    "\n",
    "            # Plot original\n",
    "            plt.subplot(2, num_images, i + 1)\n",
    "            plt.imshow(img)\n",
    "            plt.title(f\"True: {class_names[label]}\")\n",
    "            plt.axis(\"off\")\n",
    "\n",
    "            # Plot Grad-CAM overlay\n",
    "            plt.subplot(2, num_images, i + 1 + num_images)\n",
    "            plt.imshow(superimposed_img)\n",
    "            plt.title(\"Grad-CAM\")\n",
    "            plt.axis(\"off\")\n",
    "\n",
    "        break\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a65d14",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "envfirst",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
